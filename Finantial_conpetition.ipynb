{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## moduleの読み込み"
      ],
      "metadata": {
        "id": "V1-pY-qUvfBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pickle\n",
        "import gc\n",
        "import mlflow\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from umap import UMAP\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import matplotlib.pyplot as plt\n",
        "import optuna.integration.lightgbm as lgb_o\n",
        "import lightgbm as lgb\n",
        "import optuna"
      ],
      "metadata": {
        "id": "OiMDm2VJvpua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 関数の作成"
      ],
      "metadata": {
        "id": "h3yV5I6av1RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#データの読み込みからpandasのデータフレームに読み込む\n",
        "def read_db(name):\n",
        "    local = \"main.db_odl_user_838812.\"\n",
        "    df = spark.table(local + name).toPandas()\n",
        "    return df\n",
        "\n",
        "#加工したデータをデータベースに保存する\n",
        "def save_db(df,name:str):\n",
        "    now = str(datetime.date.today()).replace(\"-\",\"\")\n",
        "    spark.createDataFrame(df).write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(name + \"_\" +now)\n",
        "\n",
        "#データを加工する\n",
        "def process_data(df):\n",
        "    #数字型の日付は月バラバラで見る\n",
        "    df['account_open_m'] = df['account_open_date'].astype(str).str[-2:].astype(int)\n",
        "    df['hl_open_m'] = df['hl_open_date'].astype(str).str[-2:].astype(int)\n",
        "\n",
        "    #開設日から何週経過しているのかの特徴量を追加\n",
        "    df[\"hl_elapsed_days\"] = (pd.to_datetime(df['yyyymm'], format = \"%Y%m\") - pd.to_datetime(df['hl_open_date'], format = \"%Y%m\")).dt.days // 7\n",
        "    df[\"account_elapsed_date\"] = (pd.to_datetime(df['yyyymm'], format = \"%Y%m\") - pd.to_datetime(df['account_open_date'], format = \"%Y%m\")).dt.days // 7\n",
        "\n",
        "    # gidごとの時系列順に番号を付けた新しいカラムを追加\n",
        "    df = df.sort_values(['gid','yyyymm'],ascending=True)\n",
        "    df['order'] = df.groupby('gid').cumcount() + 1\n",
        "\n",
        "    #総収入と総支出にまとめる\n",
        "    df['income'] = df['in_amt_atm'] + df['in_amt_ft'] + df['in_amt_auto']\n",
        "    df['expenditure'] = df['out_amt_atm'] + df['out_amt_ft'] + df['out_amt_auto'] + df['hl_pmt'] + df['ln_pmt']\n",
        "\n",
        "    #預貸率 = 貸出残高/預金残高\n",
        "    df['deposit_ratio'] = df['hl_balance']/(df['eb_amt_jpycasa'] + df['eb_amt_jpytd'])\n",
        "    #年収倍率 = 貸出残高/入金金額\n",
        "    df['annual_income'] = df['hl_balance']/df['income']\n",
        "\n",
        "    df = df.replace([np.inf,-np.inf],np.nan)\n",
        "\n",
        "    df['deposit_ratio'] = df['deposit_ratio'].fillna(df['deposit_ratio'].median())\n",
        "    df['annual_income'] = df['annual_income'].fillna(df['annual_income'].median())\n",
        "\n",
        "    #毎月の利益を算出す\n",
        "    df['profit'] = df['income'] - df['expenditure']\n",
        "\n",
        "    #傾きの特徴量をmerge\n",
        "    income = read_db('income_table_20230209')\n",
        "    expenditure = read_db('expenditure_table_20230209')\n",
        "    eb_amt_jpycasa = read_db('eb_amt_jpycasa_table_20230210')\n",
        "\n",
        "    df = df.merge(income[['gid','yyyymm','income_slope','income_angle','income_profit_cumsum']],left_on=['gid','yyyymm'],right_on=['gid','yyyymm'],how='left')\n",
        "    df = df.merge(expenditure[['gid','yyyymm','expenditure_slope','expenditure_angle','expenditure_profit_cumsum']],left_on=['gid','yyyymm'],right_on=['gid','yyyymm'],how='left')\n",
        "    df = df.merge(eb_amt_jpycasa[['gid','yyyymm','eb_amt_jpycasa_slope','eb_amt_jpycasa_angle','eb_amt_jpycasa_profit_cumsum']],left_on=['gid','yyyymm'],right_on=['gid','yyyymm'],how='left')\n",
        "\n",
        "    df['income_deficit_ratio'] = df['income_profit_cumsum'] / df['order']\n",
        "    df['expenditure_deficit_ratio'] = df['expenditure_profit_cumsum'] / df['order']\n",
        "    df['eb_amt_jpycasa'] = df['eb_amt_jpycasa_profit_cumsum'] / df['order']\n",
        "\n",
        "    #使わないカラムをすべて削除\n",
        "    delete_cols = ['out_d_atm','in_d_atm','out_d_ft','in_d_ft','out_d_auto','in_d_auto','ln_contract','ln_balance','ln_delay_flag','ln_pmt','sex',#特に使えない特徴量\n",
        "                   'order','income_profit_cumsum','expenditure_profit_cumsum','eb_amt_jpycasa_profit_cumsum',#傾きのデータで不必要になったもの\n",
        "                   'out_mindate_atm','out_maxdate_atm','in_mindate_atm','in_maxdate_atm','out_mindate_ft','out_maxdate_ft','in_mindate_ft','in_maxdate_ft','out_mindate_auto','out_maxdate_auto','in_mindate_auto','in_maxdate_auto']#最小日最大日は削除\n",
        "    for col in delete_cols:\n",
        "        df.drop(col,axis=1,inplace=True)\n",
        "    return df\n",
        "\n",
        "#型を自動変換して、メモリを減らす\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose:\n",
        "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df\n",
        "\n",
        "#直近3カ月のデータを結合するための関数\n",
        "def filtered_df(df,n_months):\n",
        "    if n_months == 'all':\n",
        "        return df.sort_values('yyyymm',ascending=False)\n",
        "    else:\n",
        "        return df.sort_values('yyyymm',ascending=False).groupby('gid').head(n_months)\n",
        "\n",
        "def summarize_sum(df,target_col):\n",
        "    return df.groupby('gid')[[target_col]].sum()\n",
        "\n",
        "def summarize_mean(df,target_col):\n",
        "    return df.groupby('gid')[[target_col]].mean()\n",
        "\n",
        "def merged_past(df,n_month, target_cols):\n",
        "     #空のdict作成\n",
        "    output_dict = {}\n",
        "    separated_dict = {}\n",
        "    separated_past_dict = {}\n",
        "\n",
        "    df = df.sort_values('yyyymm',ascending=True)\n",
        "    for date,df_by_date in tqdm(df.groupby('yyyymm')):\n",
        "        separated_dict[date] = df_by_date\n",
        "        separated_past_dict[date] = df.query('yyyymm < @date')\n",
        "\n",
        "    for date in tqdm(list(separated_dict)):\n",
        "        recent = separated_dict[date].copy()\n",
        "        past = separated_past_dict[date].copy()\n",
        "        #直近nカ月の口座状態を平均する\n",
        "        for month in n_months:\n",
        "            if month == 'all':\n",
        "                for col in target_cols:\n",
        "                    past_months = summarize_sum(filtered_df(past,month),col).rename(columns={col:'{}_total'.format(col)})\n",
        "                    recent = recent.merge(past_months,left_on='gid',right_on='gid',how='left')\n",
        "                for col in target_cols:\n",
        "                    past_months = summarize_mean(filtered_df(past,month),col).rename(columns={col:'{}_avg'.format(col)})\n",
        "                    recent = recent.merge(past_months,left_on='gid',right_on='gid',how='left')\n",
        "            else:\n",
        "                for col in target_cols:\n",
        "                    past_months = summarize_mean(filtered_df(past,month),col).rename(columns={col:'{}_avg_{}M'.format(col,month)})\n",
        "                    recent = recent.merge(past_months,left_on='gid',right_on='gid',how='left')\n",
        "            output_dict[date] = recent\n",
        "        del separated_dict[date],separated_past_dict[date],recent,past,past_months\n",
        "        gc.collect()\n",
        "    data = pd.concat([output_dict[date] for date in output_dict])\n",
        "    del output_dict\n",
        "    gc.collect()\n",
        "    return data\n",
        "\n",
        "def slope_engineerring(data,col):\n",
        "    test = data[['gid','yyyymm',col]].sort_values(['gid','yyyymm'])\n",
        "\n",
        "    #その時点～直近2カ月前のデータから傾きを求める\n",
        "    ###EDAの結果、口座残高の減り方が急な契約者は延滞しやすい傾向を見つけた為、\n",
        "    ###残高の減りを方を、グラフ（二次関数）の傾きとして表現\n",
        "    concat = {}\n",
        "    for gid,df_test in tqdm(test.groupby('gid')):\n",
        "\n",
        "        test= df_test.sort_values(['gid','yyyymm'])[[col]]\n",
        "\n",
        "        df_concat = pd.concat([\n",
        "                    df_test,\n",
        "                    test.shift(1).rename(columns = {col:'{}_1M'.format(col)}),\n",
        "                    test.shift(2).rename(columns = {col:'{}_2M'.format(col)})\n",
        "                   ],axis=1).reset_index(drop=True)\n",
        "\n",
        "        # 新しいカラムを追加する\n",
        "        df_concat[\"{}slope\".format(col)] = np.nan\n",
        "\n",
        "        list = df_concat.index.values\n",
        "        # 傾きを求めて、各行に追加する\n",
        "        for i in list:\n",
        "            x = np.array([200000, 100000, 0])\n",
        "            y = np.array([df_concat.iloc[i][col], df_concat.iloc[i][\"{}_1M\".format(col)], df_concat.iloc[i][\"{}_2M\".format(col)]])\n",
        "            mask = ~np.isnan(y)\n",
        "            if np.sum(mask) >= 2:\n",
        "                a, b = np.polyfit(x[mask], y[mask], 1)\n",
        "                df_concat.at[i, \"{}_slope\".format(col)] = a\n",
        "                df_concat['{}_angle'.format(col)] = round(np.arctan(df_concat['{}_slope'.format(col)])*(180/np.pi),2)\n",
        "                df_concat['{}_profit'.format(col)] = df_concat['{}_angle'.format(col)].apply(lambda x:1 if x<0 else 0)\n",
        "                df_concat['{}_profit_cumsum'.format(col)] = np.cumsum(df_concat['{}_profit'.format(col)])\n",
        "        concat[gid] = df_concat\n",
        "    df = pd.concat(concat[gid] for gid in concat )\n",
        "    return df\n",
        "\n",
        "def features_engineering(row):\n",
        "    df = row.copy()\n",
        "    #特徴量として弱いカラムを削除\n",
        "    delete_cols = [\n",
        "                  'out_n_atm','in_n_auto','out_n_ft','in_n_ft'#merged_pastで外せなかったものをここで削除する\n",
        "                  ]\n",
        "    for col in delete_cols:\n",
        "        df = df.drop(col,axis=1)\n",
        "    #各ユーザーの初月のデータを削除\n",
        "    df = df[df['income_angle'].notnull()]\n",
        "    #NaNの処理を行っとく\n",
        "    #df = df.fillna(0)\n",
        "\n",
        "    return reduce_mem_usage(df)\n",
        "\n",
        "\n",
        "#学習するためのデータに分ける\n",
        "def split_data(df,recent,past):\n",
        "    train = df[(df['yyyymm']!=recent)&(df['yyyymm']>past)]\n",
        "    test  = df[df['yyyymm']==recent]\n",
        "    return train,test"
      ],
      "metadata": {
        "id": "E3Ow_USLv8zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#target_colmnsと前月と累計の設定場所\n",
        "n_months = [3,'all']\n",
        "target_cols = [\n",
        "'eb_amt_jpycasa',\n",
        "'income',\n",
        "'expenditure',\n",
        "'in_amt_atm',\n",
        "'in_amt_ft',\n",
        "'in_amt_auto',\n",
        "'in_n_atm',\n",
        "'in_n_ft',\n",
        "'in_n_auto',\n",
        "'out_amt_atm',\n",
        "'out_amt_ft',\n",
        "'out_amt_auto',\n",
        "'out_n_atm',\n",
        "'out_n_ft',\n",
        "'out_n_auto',\n",
        "]"
      ],
      "metadata": {
        "id": "BRmcV8MRwSM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データ前処理"
      ],
      "metadata": {
        "id": "Pljttdl8wMtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#過去データを結合\n",
        "data = merged_past(data,n_months,target_cols)\n",
        "\n",
        "#最後のデータ加工(#特徴量エンジニアリング)\n",
        "data = features_engineering(data)\n",
        "\n",
        "#加工したデータをもう一度testデータとtrainデータに分ける\n",
        "data,subject = split_data(data,202111,201908)\n",
        "#日付を最新順にする\n",
        "data = data.sort_values('yyyymm',ascending=True)"
      ],
      "metadata": {
        "id": "wMeK9u1hwYQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルの構築"
      ],
      "metadata": {
        "id": "_G_mI-PBwsIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ➀ハイパーパラメータの最適化"
      ],
      "metadata": {
        "id": "VzNPVFmTw4gZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#validまで分ける\n",
        "sample = data\n",
        "train, test = split_data(sample,recent=202110,past=201908)\n",
        "train, valid = split_data(train,recent=202109,past=201908)\n",
        "#Optunaのためにvalidを含めた変換を行う\n",
        "X_train = train.drop(['gid','yyyymm','target_flag'],axis=1)\n",
        "y_train = train['target_flag']\n",
        "X_test = test.drop(['gid','yyyymm','target_flag'],axis=1)\n",
        "y_test = test['target_flag']\n",
        "X_valid = valid.drop(['gid','yyyymm','target_flag'],axis=1)\n",
        "y_valid = valid['target_flag']\n",
        "\n",
        "lgb_train = lgb_o.Dataset(X_train.values,y_train.values)\n",
        "lgb_valid = lgb_o.Dataset(X_valid.values,y_valid.values)\n",
        "\n",
        "scale_pos_weight = ((y_train==0).count()) / ((y_train==1).count())\n",
        "\n",
        "params = {\n",
        "    'objective':'binary',\n",
        "    'scale_pos_weight':scale_pos_weight,\n",
        "    'random_state':100,\n",
        "    'metric': 'auc',\n",
        "}\n",
        "\n",
        "lgb_clf_o = lgb_o.train(params,lgb_train,\n",
        "                       valid_sets=(lgb_train,lgb_valid),\n",
        "                       verbose_eval=100,\n",
        "                       early_stopping_rounds=10,\n",
        "                       optuna_seed=100 # optunaのseed固定\n",
        "                       )"
      ],
      "metadata": {
        "id": "ALv0paY2wxDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ②クロスバリデーションを行いながらハイパーパラメータ最適化"
      ],
      "metadata": {
        "id": "HskDpbCrxSvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMRegressor, LGBMClassifier, Booster\n",
        "def objective(trial):\n",
        "    # lightgbm用のハイパーパラメータ\n",
        "    params = {\n",
        "        'boosting_type': 'gbdt',\n",
        "        'objective': 'binary',\n",
        "        'metric': 'auc',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-8, 0.1),\n",
        "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 1.0),\n",
        "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 1.0),\n",
        "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n",
        "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.9, 1.0),\n",
        "        'bagging_freq': trial.suggest_int('bagging_freq', 0.8, 1.5),\n",
        "\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples',15,20),\n",
        "        'scale_pos_weight':trial.suggest_int('scale_pos_weight',80,99),\n",
        "    }\n",
        "\n",
        "    # 時系列データのクロスバリデーション\n",
        "    time_series_cv = TimeSeriesSplit(n_splits=25)\n",
        "    for train_index, test_index in time_series_cv.split(X):\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        scale_pos_weight = ((y_train==0).sum()) / ((y_train==1).sum())\n",
        "\n",
        "        # LGBMClassifierを学習\n",
        "        model = LGBMClassifier(random_state=100,**params)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # 評価\n",
        "        y_pred = model.predict_proba(X_test)[:,1]\n",
        "        auc = roc_auc_score(y_test, y_pred)\n",
        "\n",
        "        # 評価結果を返す\n",
        "        return auc\n",
        "\n",
        "#データを分ける\n",
        "sample = data\n",
        "train, test = split_data(sample,recent=202110,past=201908)\n",
        "\n",
        "X = train.drop(['yyyymm','target_flag'],axis=1)\n",
        "y = train['target_flag']\n",
        "\n",
        "# 最適なハイパーパラメータを探索\n",
        "study = optuna.create_study()\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# 最適なハイパーパラメータを表示\n",
        "print(\"Best hyperparameters: \", study.best_params)\n"
      ],
      "metadata": {
        "id": "ihuqnWE9xaAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 通常のモデル作成（ホールドアウト検証）"
      ],
      "metadata": {
        "id": "0eifTkzvxd26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = data\n",
        "#モデルを作る\n",
        "train,test = split_data(sample,recent=202110,past=201909)\n",
        "X_train = train.drop(['gid','yyyymm','target_flag'],axis=1)\n",
        "y_train = train['target_flag']\n",
        "X_test = test.drop(['gid','yyyymm','target_flag'],axis=1)\n",
        "y_test = test['target_flag']\n",
        "\n",
        "params={\n",
        "'objective': 'binary',\n",
        " 'scale_pos_weight': 1.0,\n",
        " 'random_state': 100,\n",
        " 'metric': 'auc',\n",
        " 'feature_pre_filter': False,\n",
        " 'lambda_l1': 8.92380314653473,\n",
        " 'lambda_l2': 0.0003830290292639538,\n",
        " 'num_leaves': 66,\n",
        " 'feature_fraction': 0.4,\n",
        " 'bagging_fraction': 1.0,\n",
        " 'bagging_freq': 0,\n",
        " 'min_child_samples': 25\n",
        "}\n",
        "\n",
        "lgb_clf = lgb.LGBMClassifier(**params)\n",
        "model = lgb_clf.fit(X_train.values,y_train.values)\n",
        "\n",
        "auc_train = roc_auc_score(\n",
        "y_train,model.predict_proba(X_train)[:,1]\n",
        ")\n",
        "\n",
        "auc_test = roc_auc_score(\n",
        "y_test,model.predict_proba(X_test)[:,1]\n",
        ")\n",
        "\n",
        "importance = pd.DataFrame({\n",
        "    \"features\":X_train.columns,\n",
        "    \"importance\":model.feature_importances_\n",
        "}).sort_values(\"importance\",ascending=False)\n",
        "\n",
        "print('AUC: {:.3f}(train), {:.3f}(test)'.format(auc_train, auc_test))\n",
        "importance[:50]"
      ],
      "metadata": {
        "id": "N0MUMrFoxlVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 交差検証"
      ],
      "metadata": {
        "id": "hpkEkM2_xngV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import accuracy_score\n",
        "sample = data.sort_values('yyyymm',ascending=True)\n",
        "\n",
        "X = sample.drop(['gid','yyyymm','target_flag'],axis=1)\n",
        "y = sample['target_flag']\n",
        "\n",
        "# TimeSeriesSplitを使用してクロスバリデーションを定義する\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "#パラメータ最適化のコピペ\n",
        "params={\n",
        " 'objective': 'binary',\n",
        " 'scale_pos_weight': 82.22860176412937,\n",
        " 'random_state': 100,\n",
        " 'metric': 'auc',\n",
        " 'feature_pre_filter': False,\n",
        " 'lambda_l1': 1.0517138394360073,\n",
        " 'lambda_l2': 7.635176818135586e-07,\n",
        " 'num_leaves': 33,\n",
        " 'feature_fraction': 0.8,\n",
        " 'bagging_fraction': 1.0,\n",
        " 'bagging_freq': 0\n",
        "}\n",
        "\n",
        "# 分割されたデータを使って学習と評価\n",
        "for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # LGBMClassifierのモデル生成\n",
        "    lgb_train = lgb.Dataset(X_train, y_train)\n",
        "    model = lgb.LGBMClassifier(**params)\n",
        "\n",
        "    # モデルの訓練\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 予測と評価\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"AUC:\", auc)\n",
        "\n",
        "importance = pd.DataFrame({\n",
        "    \"features\":X.columns,\n",
        "    \"importance\":model.feature_importances_\n",
        "}).sort_values(\"importance\",ascending=False)\n",
        "importance[:50]"
      ],
      "metadata": {
        "id": "xXCSHIK7xvFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 予測"
      ],
      "metadata": {
        "id": "saCAecVlxxmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict_proba(subject.drop(['gid','yyyymm','target_flag'],axis=1))[:, 1]\n",
        "subject['prob'] = y_pred"
      ],
      "metadata": {
        "id": "nktQtUS8xzeo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
